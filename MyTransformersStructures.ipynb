{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab07fa58",
   "metadata": {},
   "source": [
    "### **Mytransformers的结构**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7df7e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F \n",
    "import math, copy , time \n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe96cbeb",
   "metadata": {},
   "source": [
    "#### **Encoder** 部分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab568c",
   "metadata": {},
   "source": [
    "##### step 1: Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3aa410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyTransformers import Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "746ecc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "vocab_size = 4\n",
    "d_model = 4\n",
    "words_vec = torch.tensor([[1,2,3]])\n",
    "embed = Embeddings(vocab_size=vocab_size,\n",
    "                   d_model = d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01f907bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.2454,  1.2795,  1.9745,  0.4878],\n",
       "          [ 0.2344,  2.9271,  1.5410, -0.7455],\n",
       "          [ 0.4110,  1.2128,  1.3450, -3.1742]]], grad_fn=<MulBackward0>),\n",
       " torch.Size([1, 3, 4]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_words_vec = embed(words_vec)\n",
    "embed_words_vec,embed_words_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c71545",
   "metadata": {},
   "source": [
    "`Embeddings`类中会初始化一个权重矩阵:\n",
    "\n",
    "$ W_{embed}: R^{vocab} \\to R^{dmodel} $\n",
    "\n",
    "将每一个`word_vec`中的向量映射到维度为`d_model`的空间中.同时，具有以下优点：\n",
    "\n",
    "- 稠密向量比 one-hot 向量更节省空间\n",
    "- 向量中包含了单词的语义信息，相似的单词在向量空间中距离更近\n",
    "- 可以被神经网络直接处理，便于训练和优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdc6ba8",
   "metadata": {},
   "source": [
    "查看权重矩阵 $W_{embed}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "308b9244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.0376, -0.0901, -0.5672, -0.1897],\n",
       "         [ 0.6227,  0.6398,  0.9872,  0.2439],\n",
       "         [ 0.1172,  1.4635,  0.7705, -0.3727],\n",
       "         [ 0.2055,  0.6064,  0.6725, -1.5871]], requires_grad=True),\n",
       " torch.Size([4, 4]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.embedding_table.weight ,embed.embedding_table.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f7e6c9",
   "metadata": {},
   "source": [
    "##### step 2: PositionEncodding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ca11ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyTransformers import PositionalEncoding\n",
    "pos_enc = PositionalEncoding(d_model=d_model,\n",
    "                             dropout=0.1\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "936f6b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.3838,  2.5328,  2.1939,  1.6532],\n",
       "          [ 0.0000,  3.8527,  0.0000,  0.2827],\n",
       "          [ 1.4670,  0.8851,  1.5166, -2.4160]]], grad_fn=<MulBackward0>),\n",
       " torch.Size([1, 3, 4]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embed_words_vec = pos_enc(embed_words_vec)\n",
    "pos_embed_words_vec,pos_embed_words_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483155dc",
   "metadata": {},
   "source": [
    "`PositionalEncoding` 的函数公式：\n",
    "\n",
    "$posenc(x) = x + PE(x.size(1))$\n",
    "\n",
    "其中,PE函数只是和x的第1维(vec_len)有关,与x本身无关."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9854afb9",
   "metadata": {},
   "source": [
    "$$\n",
    "PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE(pos, 2i + 1) = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82029f",
   "metadata": {},
   "source": [
    "##### step 3: Encoder Layer的第一层:注意力函数与残差链接函数的复合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96617d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyTransformers import LayerNorm, MultiHeadedAttention,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "863f7a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0738,  1.1409,  0.4876, -0.5546],\n",
       "         [-0.5488,  1.4962, -0.5488, -0.3987],\n",
       "         [ 0.5888,  0.2784,  0.6152, -1.4824]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = LayerNorm(features=d_model)\n",
    "norm_x = norm(pos_embed_words_vec)\n",
    "norm_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4a1f98",
   "metadata": {},
   "source": [
    "进行归一化，一方面防止梯度爆炸和稳定反向传播，另一方面在注意力机制核心公式的前提是x要尽可能满足$~N(0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aae114",
   "metadata": {},
   "source": [
    "归一化公式:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = [y_1, y_2, \\dots, y_d] = \\gamma \\odot \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "-  $\\gamma $ 和  $\\beta$  是可学习的参数，形状与  $\\mathbf{x}$ 相同\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4026975a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: tensor([[[[-0.5284,  0.0963],\n",
      "          [-0.2927, -0.2988],\n",
      "          [ 0.2366, -0.0240]],\n",
      "\n",
      "         [[ 0.4215, -0.9681],\n",
      "          [ 0.6588, -0.3928],\n",
      "          [-0.5448, -0.2104]]]], grad_fn=<TransposeBackward0>) \n",
      " torch.Size([1, 2, 3, 2])\n",
      "key: tensor([[[[-0.5284,  0.0963],\n",
      "          [-0.2927, -0.2988],\n",
      "          [ 0.2366, -0.0240]],\n",
      "\n",
      "         [[ 0.4215, -0.9681],\n",
      "          [ 0.6588, -0.3928],\n",
      "          [-0.5448, -0.2104]]]], grad_fn=<TransposeBackward0>) \n",
      " torch.Size([1, 2, 3, 2])\n",
      "value: tensor([[[[-0.5284,  0.0963],\n",
      "          [-0.2927, -0.2988],\n",
      "          [ 0.2366, -0.0240]],\n",
      "\n",
      "         [[ 0.4215, -0.9681],\n",
      "          [ 0.6588, -0.3928],\n",
      "          [-0.5448, -0.2104]]]], grad_fn=<TransposeBackward0>) \n",
      " torch.Size([1, 2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "from MyTransformers import clones\n",
    "h = 2\n",
    "d_k = d_model // h\n",
    "nbatches = norm_x.size(0)\n",
    "linears = clones(nn.Linear(d_model,d_model),4)\n",
    "query,key,value = [l(x).view(nbatches,-1,h,d_k).transpose(1,2) for l,x in zip(linears,(norm_x,norm_x,norm_x))]\n",
    "print(f'query:',query,'\\n',query.shape)\n",
    "print(f'key:',query,'\\n',query.shape)\n",
    "print(f'value:',query,'\\n',query.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe773590",
   "metadata": {},
   "source": [
    "这里将原来的d_model维向量分割成了d_k维向量，其中前三个Linear层的权重矩阵可以看成每个由d_model到d_k的线性映射矩阵横向结合.同时输出的Q,K,V也是对应每个d_k向量的横向结合.这里为了带入到attention函数中计算,把seq_len与h的交换位置,不会影响张量对应的向量."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f1433f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.2325, -0.0713],\n",
      "          [-0.2144, -0.0804],\n",
      "          [-0.1775, -0.0765]],\n",
      "\n",
      "         [[ 0.3019, -0.6203],\n",
      "          [ 0.3108, -0.5877],\n",
      "          [ 0.0853, -0.5009]]]], grad_fn=<UnsafeViewBackward0>) \n",
      " torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[0.3793, 0.3381, 0.2827],\n",
      "          [0.3435, 0.3557, 0.3008],\n",
      "          [0.3139, 0.3287, 0.3575]],\n",
      "\n",
      "         [[0.4608, 0.3336, 0.2057],\n",
      "          [0.4051, 0.3856, 0.2093],\n",
      "          [0.3191, 0.2673, 0.4136]]]], grad_fn=<SoftmaxBackward0>) \n",
      " torch.Size([1, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "x,attn = attention(query,key,value)\n",
    "print(x,'\\n',x.shape)\n",
    "print(f'{attn}','\\n',attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d40f519",
   "metadata": {},
   "source": [
    "注意力公式：\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) V \\in \\mathbb{R}^{n \\times d_v}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8f7e34",
   "metadata": {},
   "source": [
    "在多头注意力中，$Q,K,V$分别为其中一个head的，总的$Q$为$Q = [Q_1,Q_2,...,Q_h]$,同理$K,V,Attention$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11093946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1737, -0.3867, -0.3769,  0.2483],\n",
       "         [ 0.1715, -0.5241, -0.6846,  0.0751],\n",
       "         [ 0.4038, -0.3603, -0.4194,  0.6586]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.transpose(1,2).contiguous().view(nbatches,-1,h*d_k)   # 还原为之前的形状\n",
    "x = linears[-1](x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4992558",
   "metadata": {},
   "source": [
    "设置一个$R^{dmodel} \\to R^{dmodel}$的权重矩阵来保证每一个head得到较好结合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f01122c",
   "metadata": {},
   "source": [
    "残差连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfa7ee2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.9412,  4.6789,  4.0109,  3.5546],\n",
       "         [ 0.1715,  7.1812, -0.6846,  0.6406],\n",
       "         [ 3.3378,  1.4099,  2.6139, -4.1735]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pos_embed_words_vec + x\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb015e3",
   "metadata": {},
   "source": [
    "##### step 4:Encoder Layer的第二层: FF 与 残差连接的复合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15dc98bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_x = norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b61cff4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1656,  0.2626,  0.1866, -0.0804],\n",
       "          [ 0.0073,  0.3206,  0.4330,  0.0232],\n",
       "          [ 0.2733, -0.0471, -0.2253, -0.0685]]], grad_fn=<ViewBackward0>),\n",
       " torch.Size([1, 3, 4]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_ff = 128\n",
    "w_1 = nn.Linear(d_model,d_ff)\n",
    "w_2 = nn.Linear(d_ff,d_model)\n",
    "ff_x = w_2(F.relu(w_1(norm_x)))\n",
    "ff_x,ff_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07a2f20",
   "metadata": {},
   "source": [
    "给定输入  $ x \\in \\mathbb{R}^{d_{\\text{model}}} $（单个位置的向量），FFN的计算步骤为：\n",
    "\n",
    " **第一层线性变换 + ReLU激活 + 第二层线性变换**\n",
    "\n",
    " **整体公式（合并表示）**：\n",
    "$$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1) W_2 + b_2$$\n",
    "\n",
    " **关键特性**：\n",
    "1. **逐位置独立计算**：FFN对序列中的每个位置 \\( x_i \\) 独立应用相同的变换，不跨位置共享信息。\n",
    "2. **扩展-收缩结构**：隐藏层维度$  d_f $ 通常大于输入维度 再投影回原维度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b51ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x + ff_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3282706",
   "metadata": {},
   "source": [
    "##### step 5:层归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df8dfba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.1494,  1.1737,  0.3757, -0.4001],\n",
       "          [-0.5024,  1.4922, -0.6196, -0.3703],\n",
       "          [ 0.8154,  0.1678,  0.4633, -1.4465]]], grad_fn=<AddBackward0>),\n",
       " torch.Size([1, 3, 4]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_x = norm(x)\n",
    "norm_x,norm_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aca92c",
   "metadata": {},
   "source": [
    "如果还有下一层layer，则将x之间当作下一层的输入，否则经过归一化后输出,encode部分结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a04670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False, False],\n",
       "         [ True,  True, False, False, False],\n",
       "         [ True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from MyTransformers import subsequent_mask\n",
    "mask = subsequent_mask(5)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d02afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "scores = torch.ones(5,5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2051d8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
       "         [ 1.0000e+00,  1.0000e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
       "         [ 1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+09, -1.0000e+09],\n",
       "         [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+09],\n",
       "         [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = scores.masked_fill(mask == 0, -1e9)\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
