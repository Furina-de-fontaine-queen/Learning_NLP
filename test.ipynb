{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f8a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F \n",
    "import math, copy , time \n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74934c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyTransformers import Batch \n",
    "def data_gen(V,batch,nbatches):\n",
    "    'Generate random data for a src-tgt copy task.'\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "        data[:, 0] = 1 # set <s> as first element\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        yield Batch(src, tgt,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2418e63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 1 Loss: -10.468163 Tokens per Sec: 1364.029175\n",
      "Epoch Step: 1 Loss: -12.888931 Tokens per Sec: 2313.796143\n",
      "tensor(-12.8900)\n",
      "Epoch Step: 1 Loss: -12.573231 Tokens per Sec: 1729.248047\n",
      "Epoch Step: 1 Loss: -13.301868 Tokens per Sec: 2468.878174\n",
      "tensor(-13.3061)\n",
      "Epoch Step: 1 Loss: -13.043589 Tokens per Sec: 1804.478882\n",
      "Epoch Step: 1 Loss: -13.423935 Tokens per Sec: 2430.708252\n",
      "tensor(-13.4216)\n",
      "Epoch Step: 1 Loss: -13.190002 Tokens per Sec: 1819.400269\n",
      "Epoch Step: 1 Loss: -13.440957 Tokens per Sec: 2424.165039\n",
      "tensor(-13.4408)\n",
      "Epoch Step: 1 Loss: -13.220473 Tokens per Sec: 1831.086670\n",
      "Epoch Step: 1 Loss: -13.483849 Tokens per Sec: 2319.231201\n",
      "tensor(-13.4840)\n",
      "Epoch Step: 1 Loss: -13.250735 Tokens per Sec: 1842.858032\n",
      "Epoch Step: 1 Loss: -13.481214 Tokens per Sec: 2515.634766\n",
      "tensor(-13.4810)\n",
      "Epoch Step: 1 Loss: -13.277815 Tokens per Sec: 1797.797119\n",
      "Epoch Step: 1 Loss: -13.487893 Tokens per Sec: 2489.611572\n",
      "tensor(-13.4879)\n",
      "Epoch Step: 1 Loss: -13.260745 Tokens per Sec: 1803.862305\n",
      "Epoch Step: 1 Loss: -13.474751 Tokens per Sec: 2489.584229\n",
      "tensor(-13.4746)\n",
      "Epoch Step: 1 Loss: -13.219271 Tokens per Sec: 1906.407104\n",
      "Epoch Step: 1 Loss: -13.460563 Tokens per Sec: 2315.416504\n",
      "tensor(-13.4605)\n",
      "Epoch Step: 1 Loss: -13.311812 Tokens per Sec: 1807.210205\n",
      "Epoch Step: 1 Loss: -13.481725 Tokens per Sec: 2488.380859\n",
      "tensor(-13.4818)\n"
     ]
    }
   ],
   "source": [
    "from MyTransformers import LabelSmoothing,make_model,NoamOpt,run_epoch,SimpleLossCompute\n",
    "V = 11 \n",
    "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.01)\n",
    "model = make_model(V,V,N=2,d_model=512, d_ff=2048, h=8, dropout=0.3)\n",
    "model_opt = NoamOpt(model_size=model.src_embed[0].d_model,\n",
    "                    factor= 1, \n",
    "                    warmup=400,\n",
    "                    optimizer=torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    run_epoch(data_iter=data_gen(V,30,20),\n",
    "              model=model,\n",
    "              loss_compute=SimpleLossCompute(generator=model.generator,\n",
    "                                              criterion=criterion,opt=model_opt))\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    print(run_epoch(data_iter=data_gen(V,30,5),\n",
    "                    model=model,\n",
    "                    loss_compute=SimpleLossCompute(generator=model.generator,\n",
    "                                                    criterion=criterion,opt=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad73fc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[-11.8144,  -2.2506,  -2.1323,  -2.3044,  -2.3096,  -2.4100,  -2.4081,\n",
      "          -2.1976,  -2.2582,  -2.2997,  -2.5098]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[1, 2]])\n",
      "1 tensor([[-11.9322,  -2.2235,  -2.1459,  -2.3279,  -2.3237,  -2.3995,  -2.4132,\n",
      "          -2.1613,  -2.2526,  -2.3223,  -2.5163]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[1, 2, 2]])\n",
      "2 tensor([[-11.9398,  -2.2198,  -2.1548,  -2.3203,  -2.3282,  -2.4041,  -2.4141,\n",
      "          -2.1597,  -2.2549,  -2.3128,  -2.5171]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[1, 2, 2, 2]])\n",
      "3 tensor([[-11.9444,  -2.2165,  -2.1579,  -2.3222,  -2.3259,  -2.4075,  -2.4215,\n",
      "          -2.1575,  -2.2515,  -2.3125,  -2.5136]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[1, 2, 2, 2, 7]])\n",
      "4 tensor([[-11.7707,  -2.2121,  -2.1534,  -2.3234,  -2.3043,  -2.4408,  -2.3837,\n",
      "          -2.1928,  -2.2736,  -2.2749,  -2.5254]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[1, 2, 2, 2, 7, 2]])\n",
      "5 tensor([[-11.9389,  -2.2145,  -2.1527,  -2.3403,  -2.3146,  -2.4003,  -2.4308,\n",
      "          -2.1462,  -2.2518,  -2.3247,  -2.5146]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[1, 2, 2, 2, 7, 2, 7]])\n",
      "6 tensor([[-11.7623,  -2.2136,  -2.1516,  -2.3338,  -2.3011,  -2.4328,  -2.3808,\n",
      "          -2.1828,  -2.2783,  -2.2814,  -2.5292]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[1, 2, 2, 2, 7, 2, 7, 2]])\n",
      "7 tensor([[-11.9334,  -2.2165,  -2.1525,  -2.3411,  -2.3139,  -2.4003,  -2.4211,\n",
      "          -2.1391,  -2.2693,  -2.3191,  -2.5171]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[1, 2, 2, 2, 7, 2, 7, 2, 7]])\n",
      "8 tensor([[-11.7635,  -2.2179,  -2.1594,  -2.3295,  -2.2996,  -2.4527,  -2.3743,\n",
      "          -2.1782,  -2.2890,  -2.2644,  -2.5198]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[1, 2, 2, 2, 7, 2, 7, 2, 7, 2]])\n",
      "tensor([[1, 2, 2, 2, 7, 2, 7, 2, 7, 2]])\n"
     ]
    }
   ],
   "source": [
    "from MyTransformers import subsequent_mask\n",
    "def greedy_decode(model, src, src_mask,max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, Variable(ys), Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        print(i,prob)\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "        print(ys)\n",
    "    return ys\n",
    "model.eval()\n",
    "src = Variable(torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]))\n",
    "src_mask = Variable(torch.ones(1, 1, 10))\n",
    "print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
